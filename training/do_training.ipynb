{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pymysql"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Python 3.9.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.feature_selection as fs\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import normalize\n",
    "from sklearn.svm import SVR, NuSVR\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ysi = pd.read_csv(\"../datasets/ysi_dataset.csv\")\n",
    "weather = pd.read_csv(\"../datasets/weather_dataset.csv\")\n",
    "# convert to date time and round ysi to nearest 15 mins\n",
    "ysi['datetime'] = pd.to_datetime(ysi['datetime']).dt.round('15min')\n",
    "weather['datetime'] = pd.to_datetime(weather['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'experimentid_x', 'ph', 'temperature_oc',\n",
       "       'par_umol_photons_m2_s', 'dissolved_oxygen_mg_l', 'airtemp_oc',\n",
       "       'global_light_energy_w_m2', 'humid_rh', 'wdspd_m_s'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine both DF's on datetime\n",
    "combined = pd.merge(ysi, weather, on='datetime')\n",
    "combined.reset_index(drop=True, inplace=True)\n",
    "combined = combined.drop(columns=['experimentid_y'])\n",
    "combined.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined = combined.drop(combined[combined['ph'] < 5].index)\n",
    "# combined = combined.drop(combined[combined['ph'] > 9].index)\n",
    "combined = combined.drop(combined[combined['dissolved_oxygen_mg_l'] < 2].index)\n",
    "combined = combined.drop(combined[combined['dissolved_oxygen_mg_l'] > 20].index)\n",
    "combined = combined.drop(combined[combined['dissolved_oxygen_mg_l'] < 3/100 * combined['global_light_energy_w_m2']].index)\n",
    "combined = combined.drop(combined[combined['dissolved_oxygen_mg_l'] >( 3/100 * combined['global_light_energy_w_m2']) + 15].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv(\"../datasets/do_combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = combined.sample(3000)\n",
    "plt.plot(feat['temperature_oc'],feat['dissolved_oxygen_mg_l'], 'bo')\n",
    "plt.ylabel(\"ph\")\n",
    "plt.xlabel(\"global_light_energy_w_m2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv(\"../datasets/do_combined.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feat[['ph', 'temperature_oc', 'global_light_energy_w_m2', 'humid_rh']].values\n",
    "y = feat['dissolved_oxygen_mg_l'].values\n",
    "regressor = RandomForestRegressor(n_estimators=50)\n",
    "regressor.fit(X, y)\n",
    "\n",
    "\n",
    "print(regressor.feature_importances_)\n",
    "\n",
    "model = SelectFromModel(regressor, prefit=True)\n",
    "X_new = model.transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features selected: ph, temperature_oc, global_light_energy_w_m2, humid_rh\n",
    "not considered: wdspd_m_s (low correlation), par_umol_photons_m2_s (duplicated),"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['datetime', 'experimentid_x', 'ph', 'temperature_oc',\n",
      "       'par_umol_photons_m2_s', 'dissolved_oxygen_mg_l', 'airtemp_oc',\n",
      "       'global_light_energy_w_m2', 'humid_rh', 'wdspd_m_s', 'wdspd_m_s_ph',\n",
      "       'wdspd_m_s_airtemp_oc_ph', 'airtemp_oc_ph',\n",
      "       'log_ph_global_light_energy_w_m2',\n",
      "       'log_temperature_oc_global_light_energy_w_m2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# store all rows where temp is greater than 25 in splitted\n",
    "splitted = pd.read_csv(\"../../../../../../Downloads/do_combined_updated_new.csv\")\n",
    "print(splitted.columns)\n",
    "test = splitted.copy(True)\n",
    "X_train,X_test,y_train,y_test = train_test_split(test[['global_light_energy_w_m2', 'ph', 'wdspd_m_s_airtemp_oc_ph', 'humid_rh']], \n",
    "        test['dissolved_oxygen_mg_l'], test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "\n",
    "def stats(y_pred_all, y_test_all):\n",
    "    # Calculate the R2 score\n",
    "    r2 = r2_score(y_test_all, y_pred_all)\n",
    "\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y_test_all, y_pred_all)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    mse = mean_squared_error(y_test_all, y_pred_all, squared=False)\n",
    "\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Linear Regression ------------\n",
      "R2 Score: 0.6737\n",
      "Mean Absolute Error (MAE): 1.9003851362098825\n",
      "Root Mean Squared Error (RMSE): 1.5464444546748186\n"
     ]
    }
   ],
   "source": [
    "print(\"------------ Linear Regression ------------\")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "y_pred = linreg.predict(X_test)\n",
    "stats(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Exponential SVR Results ------------\n",
      "R2 Score: 0.7777\n",
      "Mean Absolute Error (MAE): 1.382567517011247\n",
      "Root Mean Squared Error (RMSE): 1.3940193766137419\n",
      "------------ Exponential SVR Results ------------\n",
      "R2 Score: 0.7700\n",
      "Mean Absolute Error (MAE): 1.3942909200906513\n",
      "Root Mean Squared Error (RMSE): 1.4059255679507698\n",
      "------------ Polynomial SVR Results ------------\n",
      "R2 Score: 0.7828\n",
      "Mean Absolute Error (MAE): 1.3479054239422932\n",
      "Root Mean Squared Error (RMSE): 1.385961798084692\n"
     ]
    }
   ],
   "source": [
    "print(\"------------ Exponential SVR Results ------------\")\n",
    "clf_rbf_nusvm = make_pipeline(preprocessing.SplineTransformer(), NuSVR(kernel='rbf', shrinking=True, C=1.5))\n",
    "clf_rbf_nusvm.fit(X_train, y_train)\n",
    "y_pred = clf_rbf_nusvm.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------------ Exponential SVR Results ------------\")\n",
    "clf_rbf_svm = make_pipeline(preprocessing.SplineTransformer(), SVR(kernel='rbf', shrinking=True, C=1.5))\n",
    "clf_rbf_svm.fit(X_train[:len(X_train)//2], y_train[:len(X_train)//2])\n",
    "clf_rbf_svm.fit(X_train[len(X_train)//2:], y_train[len(X_train)//2:])\n",
    "y_pred = clf_rbf_svm.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------------ Polynomial SVR Results ------------\")\n",
    "clf_poly_nusvm = make_pipeline(preprocessing.SplineTransformer(), NuSVR(kernel='poly', shrinking=False, C=2.5))\n",
    "clf_poly_nusvm.fit(X_train, y_train)\n",
    "y_pred = clf_poly_nusvm.predict(X_test)\n",
    "stats(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Bagging Results ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.8656\n",
      "Mean Absolute Error (MAE): 0.9812385261797028\n",
      "Root Mean Squared Error (RMSE): 1.2292702286440744\n",
      "------------ Stacking Results ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.8706\n",
      "Mean Absolute Error (MAE): 0.9715637935788428\n",
      "Root Mean Squared Error (RMSE): 1.217751364980006\n"
     ]
    }
   ],
   "source": [
    "print(\"------------ Bagging Results ------------\")\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "base_estimator = DecisionTreeRegressor()\n",
    "bag_pipe = make_pipeline(preprocessing.SplineTransformer(), BaggingRegressor(base_estimator=base_estimator, n_estimators=10, n_jobs=5))\n",
    "bag_pipe.fit(X_train, y_train)\n",
    "y_pred = bag_pipe.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "\n",
    "print(\"------------ Stacking Results ------------\")\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "model = StackingRegressor(estimators=[\n",
    "    ('svr', NuSVR(kernel='poly', shrinking=False, C=2.5)), \n",
    "    ('rf', RandomForestRegressor(n_estimators=10,random_state=42, n_jobs=8)),\n",
    "    ('bag', BaggingRegressor(n_jobs=5)),\n",
    "    ('bst', GradientBoostingRegressor(learning_rate=0.1, loss='huber', max_depth=6, criterion='squared_error')),\n",
    "    ('n1', MLPRegressor(hidden_layer_sizes=(2,3), activation='relu')),\n",
    "    ('n2', MLPRegressor(hidden_layer_sizes=(3,2), activation='tanh'))\n",
    "])\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "stats(y_pred, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Random Forest Results ------------\n",
      "R2 Score: 0.8662\n",
      "Mean Absolute Error (MAE): 0.9578322337011381\n",
      "Root Mean Squared Error (RMSE): 1.227838800319483\n",
      "------------ Poly Regression Results ------------\n",
      "R2 Score: 0.7636\n",
      "Mean Absolute Error (MAE): 1.3946363812143787\n",
      "Root Mean Squared Error (RMSE): 1.4156985214599742\n",
      "------------ Decision Tree Results ------------\n",
      "R2 Score: 0.7815\n",
      "Mean Absolute Error (MAE): 1.1219497987830105\n",
      "Root Mean Squared Error (RMSE): 1.3880192102364317\n",
      "------------ XG Boost Results ------------\n",
      "R2 Score: 0.8436\n",
      "Mean Absolute Error (MAE): 1.1239813975564932\n",
      "Root Mean Squared Error (RMSE): 1.2767018004050568\n"
     ]
    }
   ],
   "source": [
    "print(\"------------ Random Forest Results ------------\")\n",
    "rf_regressor = make_pipeline(preprocessing.SplineTransformer(), RandomForestRegressor(n_estimators=24, max_depth=20))\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------------ Poly Regression Results ------------\")\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=5, include_bias=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "# Initialize and fit the linear regression model\n",
    "poly_reg = LinearRegression()\n",
    "poly_reg.fit(X_train_poly, y_train)\n",
    "# Predict the target variable for training and test sets\n",
    "y_pred = poly_reg.predict(X_test_poly)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------------ Decision Tree Results ------------\")\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "d_tree = DecisionTreeRegressor(max_depth=24)\n",
    "d_tree.fit(X_train, y_train)\n",
    "y_pred = d_tree.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------------ XG Boost Results ------------\")\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "xg_boost = GradientBoostingRegressor(learning_rate=0.1, loss='huber', max_depth=6, criterion='squared_error')\n",
    "xg_boost.fit(X_train, y_train)\n",
    "y_pred = xg_boost.predict(X_test)\n",
    "stats(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3, 16),\n",
    "    nn.CELU(),\n",
    "    nn.Linear(16, 16),\n",
    "    nn.CELU(),\n",
    "    nn.Linear(16, 4),\n",
    "    nn.CELU(),\n",
    "    nn.Linear(4, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.numeric_columns = ['log_temperature_oc_global_light_energy_w_m2', 'log_ph_global_light_energy_w_m2', 'wdspd_m_s_airtemp_oc_ph']\n",
    "        self.transformer = preprocessing.QuantileTransformer()\n",
    "        transformed_data = self.transformer.fit_transform(dataframe[self.numeric_columns].values)\n",
    "        dataframe[self.numeric_columns] = transformed_data\n",
    "        self.data = dataframe.dropna(subset=['dissolved_oxygen_mg_l']).reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def getInverseTransform(self, inputs):\n",
    "        return self.quantile_transformer.inverse_transform(inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.data[self.numeric_columns].iloc[idx].values.astype(np.float32)\n",
    "        label = self.data['dissolved_oxygen_mg_l'].iloc[idx].astype(np.float32)\n",
    "        inputs = torch.tensor(inputs)\n",
    "        label = torch.tensor(label)\n",
    "        return inputs, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 5.4914\n",
      "Epoch 2, Training Loss: 2.5774\n",
      "Epoch 3, Training Loss: 2.3107\n",
      "Epoch 4, Training Loss: 2.0774\n",
      "Epoch 5, Training Loss: 1.8840\n",
      "Epoch 6, Training Loss: 1.7336\n",
      "Epoch 7, Training Loss: 1.6207\n",
      "Epoch 8, Training Loss: 1.5535\n",
      "Epoch 9, Training Loss: 1.5177\n",
      "Epoch 10, Training Loss: 1.5021\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.SmoothL1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "splitted = pd.read_csv(\"../datasets/Philly/new_features.csv\")\n",
    "test = splitted.copy(True)\n",
    "trainset, testset = train_test_split(test, test_size=0.1)\n",
    "\n",
    "data_loader = DataLoader(MyDataset(trainset), shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for inputs, labels in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # Reshape labels to match the shape of outputs\n",
    "        labels = labels.view(outputs.shape)\n",
    "\n",
    "        loss = loss_fn(outputs , labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    # Print the loss for each epoch\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {epoch_loss:.4f}\")\n",
    "# Training complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "torch.save(model.state_dict(), './weights/weights2.sav')\n",
    "model.load_state_dict(torch.load('./weights/weights2.sav'))\n",
    "\n",
    "# weights 1 : loss = 30.98\n",
    "# weights 2 : loss = 1.1218"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store true and predicted values\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "newset = pd.merge(ysi, weather, on='datetime').sample(1000)\n",
    "\n",
    "dataset = MyDataset(testset)\n",
    "data_loader = DataLoader(dataset)\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    for inputs, label in data_loader:\n",
    "\n",
    "        # Forward pass to get predictions\n",
    "        y_pred = model(inputs)\n",
    "\n",
    "        # Convert the predictions and targets to numpy arrays\n",
    "        y_pred_np = y_pred.numpy()\n",
    "        label_np = label.numpy()\n",
    "        #print(dataset.quantile_transformer.inverse_transform(inputs.numpy()))\n",
    "        #print(f\"actual: { y_pred.item():3f} predicted: {label.item():3f}\")\n",
    "\n",
    "        # Append batch results to the overall lists\n",
    "        y_test_all.append(label_np)\n",
    "        y_pred_all.append(y_pred_np)\n",
    "\n",
    "# Concatenate the lists into a single array\n",
    "y_test_all = np.concatenate(y_test_all)\n",
    "y_pred_all = np.concatenate(y_pred_all)\n",
    "\n",
    "# Calculate the R2 score\n",
    "r2 = r2_score(y_test_all, y_pred_all)\n",
    "\n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test_all, y_pred_all)\n",
    "\n",
    "# Calculate RMSE\n",
    "mse = mean_squared_error(y_test_all, y_pred_all, squared=False)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Root Mean Squared Error (RMSE):\", math.sqrt(mse))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre - Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play = nn.Sequential(\n",
    "    nn.Linear(4, 8),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(8, 8),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "\n",
    "play.load_state_dict(torch.load('weights.sav'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import json\n",
    "import io\n",
    "\n",
    "# Save the trained model\n",
    "model_filename = './weights/do_rf.pkl'\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Load the saved model\n",
    "with open(model_filename, 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "# Specify the name of the pickle file and the .mar archive file\n",
    "mar_filename = 'model_archive.mar'\n",
    "\n",
    "# Create a new tar archive\n",
    "with tarfile.open(mar_filename, 'w') as archive:\n",
    "    # Add the pickle file to the archive\n",
    "    archive.add(model_filename)\n",
    "\n",
    "# Create a manifest file with the required metadata\n",
    "manifest = {'model-file': model_filename, 'model-name': 'RandomForestRegressor'}\n",
    "\n",
    "# Add the manifest file to the archive\n",
    "with tarfile.open(mar_filename, 'a') as archive:\n",
    "    manifest_string = json.dumps(manifest)\n",
    "    manifest_bytes = manifest_string.encode('utf-8')\n",
    "    manifest_file = tarfile.TarInfo('MANIFEST')\n",
    "    manifest_file.size = len(manifest_bytes)\n",
    "    archive.addfile(manifest_file, io.BytesIO(manifest_bytes))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check performance on DB values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:762: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "778"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymysql\n",
    "\n",
    "# Connect to the database\n",
    "connection = pymysql.connect(\n",
    "    host='localhost', \n",
    "    user='root', \n",
    "    password='N@wid2003', \n",
    "    db='dma_iot_morefish_spark_farms_v3'\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Get the latest data from the database and store it in a pandas dataframe\n",
    "query = \"SELECT dvd_ph, dvd_temp, dvd_updated_at, dvd_do FROM device_devicedata WHERE dvd_ph > 0 AND dvd_temp > 0 AND dvd_dev_id = 2 ORDER BY dvd_updated_at ASC\"\n",
    "df = pd.read_sql(query, connection)\n",
    "\n",
    "# Close the connection\n",
    "connection.close()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'dvd_ph': 'ph', 'dvd_temp': 'temperature', 'dvd_updated_at': 'datetime', 'dvd_do': 'do_linreg'}, inplace=True)\n",
    "df['rounded_datetime'] = pd.to_datetime(df['datetime']).dt.round('H')\n",
    "# spark_weather = pd.read_csv(\"../../../../../../Downloads/sparkfarmsweather.csv\")\n",
    "spark_weather = pd.read_csv(\"../datasets/sparkfarmsweather.csv\")\n",
    "spark_weather['time'] = pd.to_datetime(spark_weather['time'])\n",
    "spark_weather.rename(columns={'time': 'datetime'}, inplace=True)\n",
    "# Merge the weather data with the main DataFrame based on 'rounded_datetime' column\n",
    "df = df.merge(spark_weather, left_on='rounded_datetime', right_on='datetime', how='left')\n",
    "df = df.drop(columns=['datetime_y'])\n",
    "df.rename(columns={'datetime_x': 'datetime', 'diffuse_radiation (W/m²)': 'light', 'relativehumidity_2m (%)': 'humid', 'temperature_2m (°C)': 'airtemp' }, inplace=True)\n",
    "# drop all NaN values inside diffuse radiation\n",
    "df = df.dropna(subset=['light'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ph', 'temperature', 'datetime', 'do_linreg', 'rounded_datetime',\n",
       "       'airtemp', 'humid', 'precipitation (mm)', 'pressure_msl (hPa)',\n",
       "       'direct_radiation (W/m²)', 'light'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- humid\n",
      "- light\n",
      "- temperature\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- global_light_energy_w_m2\n",
      "- humid_rh\n",
      "- temperature_oc\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ny_test_all = []\\ny_pred_all = []\\n\\ndataset = MyDataset(x_train)\\ndata_loader = DataLoader(dataset)\\n\\n# Disable gradient calculation\\nwith torch.no_grad():\\n    for inputs, label in data_loader:\\n\\n        # Forward pass to get predictions\\n        y_pred = model(inputs)\\n\\n        # Convert the predictions and targets to numpy arrays\\n        y_pred_np = y_pred.numpy()\\n        y_pred_all.append(y_pred_np)\\n\\n# Concatenate the lists into a single array\\ny_pred_all = np.concatenate(y_pred_all)\\ndf['DeepNN'] = y_pred_all\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = df[['ph', 'temperature', 'light', 'humid']]\n",
    "y_pred = rf_regressor.predict(x_train)\n",
    "df['RF'] = y_pred\n",
    "\n",
    "# test the neural net model and store its result in the pandas df\n",
    "'''\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "dataset = MyDataset(x_train)\n",
    "data_loader = DataLoader(dataset)\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    for inputs, label in data_loader:\n",
    "\n",
    "        # Forward pass to get predictions\n",
    "        y_pred = model(inputs)\n",
    "\n",
    "        # Convert the predictions and targets to numpy arrays\n",
    "        y_pred_np = y_pred.numpy()\n",
    "        y_pred_all.append(y_pred_np)\n",
    "\n",
    "# Concatenate the lists into a single array\n",
    "y_pred_all = np.concatenate(y_pred_all)\n",
    "df['DeepNN'] = y_pred_all\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "myDates = [datetime(2012,1,i+3) for i in range(10)]\n",
    "myValues = [5,6,4,3,7,8,1,2,5,4]\n",
    "fig, ax = plt.subplots()\n",
    "bet = df.loc[df['datetime'].between('2023-06-09', '2023-06-13')]\n",
    "ax.plot(bet['datetime'], bet['SVM'].astype(float), 'ro')\n",
    "\n",
    "myFmt = DateFormatter(\"%m %d %H:%M\")\n",
    "ax.xaxis.set_major_formatter(myFmt)\n",
    "\n",
    "## Rotate date labels automatically\n",
    "fig.autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_data = pd.read_csv(\"../datasets/GEMStat/India-Lake-2023-07-04_13-45/samples.csv\", sep=';')\n",
    "\n",
    "# Combine 'Date' and 'Time' columns into a single column\n",
    "bd_data['datetime'] = pd.to_datetime(bd_data['Date'] + ' ' + bd_data['Time'], format='%Y-%m-%d %H:%M')\n",
    "\n",
    "# Drop the original 'Date' and 'Time' columns if no longer needed\n",
    "bd_data.drop(['Date', 'Time'], axis=1, inplace=True)\n",
    "\n",
    "# Merge by datetime and LocationID\n",
    "new_bd_data = pd.DataFrame()\n",
    "new_bd_data[['LocationID','datetime', 'O2-Dis', ]] = bd_data[bd_data['Parameter'] == 'O2-Dis'][['LocationID', 'datetime','Value']]\n",
    "\n",
    "print(bd_data.columns)\n",
    "\n",
    "new_bd_data = pd.merge(new_bd_data, bd_data[bd_data['Parameter'] == 'pH'][['datetime', 'Value', 'LocationID']], on=['datetime', 'LocationID'], how='right')\n",
    "new_bd_data = pd.merge(new_bd_data, bd_data[bd_data['Parameter'] == 'TEMP'][['datetime', 'Value', \"LocationID\"]], on=['datetime','LocationID'], how='right')\n",
    "\n",
    "new_bd_data.rename(columns={'O2-Dis': 'dissolved_oxygen_mg_l', 'Value_x': 'ph', 'Value_y': 'temperature_oc'}, inplace=True)\n",
    "\n",
    "print(len(new_bd_data))\n",
    "new_bd_data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine with Ammonia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=40, max_features='log2')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "cis = pd.read_csv(\"../datasets/upsampled_data.csv\")\n",
    "test = cis.copy(True)\n",
    "X_train,X_test,y_train,y_test = train_test_split(test[['DO_MGL','PH','precipitation (mm)','pressure_msl (hPa)', 'direct_radiation (W/m²)']], \n",
    "        test['decreaseFeed'], test_size=0.001)\n",
    "\n",
    "# Training\n",
    "clf = RandomForestClassifier(max_depth=40, criterion='gini', max_features='log2')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- RF\n",
      "- ph\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- DO_MGL\n",
      "- PH\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "'''\n",
    "1. Remove outliers greater than 2 std devs on df\n",
    "2. Impute missing values with median\n",
    "3. Apply StandardScaler to DO, PH, pressure, direct radiation\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#imputer = SimpleImputer(strategy='median')\n",
    "#df = imputer.fit_transform(df)\n",
    "nh4_test = df.copy(True)\n",
    "scaler = StandardScaler()\n",
    "nh4_test[['RF', 'ph', 'pressure_msl (hPa)', 'direct_radiation (W/m²)']] = scaler.fit_transform(nh4_test[['RF', 'ph', 'pressure_msl (hPa)', 'direct_radiation (W/m²)']])\n",
    "\n",
    "amm_pred = clf.predict(nh4_test[['RF', 'ph', 'precipitation (mm)', 'pressure_msl (hPa)', 'direct_radiation (W/m²)']])\n",
    "df['ammonia'] = amm_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ph         temperature                   datetime  \\\n",
      "746                7.67               31.31 2023-06-18 22:26:23.707350   \n",
      "522   7.987808965228319  31.578103749527713 2023-06-15 00:52:43.084418   \n",
      "703  7.9073732718894005    31.9436144294844 2023-06-18 10:07:14.818315   \n",
      "95    9.113908671973187   32.58740838275526 2023-06-09 10:23:24.981244   \n",
      "508    8.02802681189778   32.12700789204939 2023-06-14 19:56:47.966372   \n",
      "163   7.826937578550482   31.39597288319152 2023-06-10 07:27:11.153185   \n",
      "8     8.532383745286966   33.83710149075512 2023-06-08 09:28:23.101344   \n",
      "357   9.033472978634268     31.669324029471 2023-06-12 15:19:06.523164   \n",
      "462    8.67151235860913  31.578103749527713 2023-06-14 04:01:19.195079   \n",
      "531   7.947591118558858   31.48698698161479 2023-06-15 03:20:29.375885   \n",
      "\n",
      "    do_linreg    rounded_datetime  airtemp  humid  precipitation (mm)  \\\n",
      "746     9.171 2023-06-18 22:00:00     28.4     93                 0.0   \n",
      "522     9.476 2023-06-15 01:00:00     29.9     89                 0.0   \n",
      "703      9.53 2023-06-18 10:00:00     31.6     79                 0.0   \n",
      "95      8.482 2023-06-09 10:00:00     27.9     96                 1.3   \n",
      "508      9.51 2023-06-14 20:00:00     31.3     78                 0.0   \n",
      "163     8.757 2023-06-10 07:00:00     25.8     91                 0.0   \n",
      "8       8.758 2023-06-08 09:00:00     31.8     84                 0.0   \n",
      "357     8.803 2023-06-12 15:00:00     31.8     76                 1.9   \n",
      "462     8.526 2023-06-14 04:00:00     28.1     89                 0.0   \n",
      "531     9.105 2023-06-15 03:00:00     29.3     89                 0.0   \n",
      "\n",
      "     pressure_msl (hPa)  direct_radiation (W/m²)  light         RF  ammonia  \n",
      "746              1003.9                      0.0    0.0   2.876250        1  \n",
      "522               999.8                      0.0    0.0   3.890417        1  \n",
      "703              1004.0                     91.0  311.0  15.150417        1  \n",
      "95               1001.5                      0.0  110.0   8.112500        0  \n",
      "508               998.7                      0.0    0.0   3.965000        1  \n",
      "163              1000.2                      1.0   49.0   4.807500        1  \n",
      "8                1002.4                     42.0  173.0  13.122083        1  \n",
      "357               998.5                     82.0  276.0  13.801250        0  \n",
      "462               998.0                      0.0    0.0   4.679583        1  \n",
      "531               998.4                      0.0    0.0   3.391667        1  \n"
     ]
    }
   ],
   "source": [
    "print(df.sample(10))\n",
    "df.to_csv('../datasets/ammonia_preds.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

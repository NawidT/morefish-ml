{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install geopandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.feature_selection as fs\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, StackingClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.metrics import confusion_matrix, mean_absolute_error, mean_squared_error\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import normalize\n",
    "import math\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141431\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../../../../../../Downloads/UKRiverData.csv\")\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253752"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather = pd.read_csv(\"../../../../../../Downloads/archive.csv\")\n",
    "weather['time'] = pd.to_datetime(weather['time'])\n",
    "len(weather)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90393\n"
     ]
    }
   ],
   "source": [
    "data = data.dropna(subset=['DO_MGL', 'NH4_N_MGL', 'PH', 'NO2_N_MGL'])\n",
    "# Convert 'Date' and 'Time' columns to datetime\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['Time'] = pd.to_datetime(data['Time'])\n",
    "\n",
    "# Combine 'Date' and 'Time' columns into 'DateTime' column\n",
    "data['DateTime'] = data['Date'] + pd.to_timedelta(data['Time'].dt.strftime('%H:%M:%S'))\n",
    "data = data.rename(columns={'DateTime': 'time'})\n",
    "data = data.drop_duplicates(subset=['time'])\n",
    "data['time'] = data['time'].dt.round('H')\n",
    "print(len(data))\n",
    "\n",
    "# Remove 'Date' and 'Time' columns if no longer needed\n",
    "data = data.drop(['Site_Code', 'Site_Status_21Oct2020', 'OBJECTID', 'Station_Name', 'RWB_ID_RBP2', 'FESOL1_UGL',\n",
    "                  'P_SOL_MGL', 'SS_MGL','ZN_SOL_UGL', 'GlobalID','Primary_Basin', 'Depth', 'ALK_MGL', 'BOD_MGL', \n",
    "                  'COND_USCM', 'CUSOL1_MGL', 'CUSOL2_UGL', 'Date', 'Time'], axis=1)\n",
    "\n",
    "# Combine the dataframs using merge function\n",
    "weather['time'] = weather['time'].dt.tz_localize('UTC')\n",
    "comb = pd.merge(data, weather, on=['time'], how='left')\n",
    "\n",
    "# Convert our problem to a classification problem\n",
    "comb['decreaseFeed'] = 1\n",
    "comb.loc[comb['NH4_N_MGL'] <= 0.1, 'decreaseFeed'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = comb.sample(2000)\n",
    "dec = x[x['decreaseFeed'] == 1]\n",
    "man = x[x['decreaseFeed'] == 0]\n",
    "plt.plot(dec['NO3_N_MGL'], dec['NO2_N_MGL'], 'ro')\n",
    "plt.plot(man['NO3_N_MGL'], man['NO2_N_MGL'], 'bo')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10251899 0.12441052 0.1698491  0.12720548 0.10725873 0.36875718]\n"
     ]
    }
   ],
   "source": [
    "samp = comb.sample(5000)\n",
    "X = samp[['DO_MGL','PH','temperature_2m (°C)', 'pressure_msl (hPa)', 'diffuse_radiation (W/m²)']].values\n",
    "y = samp['decreaseFeed'].values\n",
    "regressor = RandomForestRegressor(n_estimators=50)\n",
    "regressor.fit(X, y)\n",
    "\n",
    "print(regressor.feature_importances_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression/Classification Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split testset into training and tetsing\n",
    "test = comb.copy(True)\n",
    "X_train,X_test,y_train,y_test = train_test_split(test[['DO_MGL','PH','temperature_2m (°C)','pressure_msl (hPa)', 'diffuse_radiation (W/m²)']], \n",
    "        test['decreaseFeed'], test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(y_pred, y_test):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "    tp = cm.data[0, 0]\n",
    "    tn = cm.data[0, 1]\n",
    "    fp = cm.data[1, 0]\n",
    "    fn = cm.data[1, 1]\n",
    "\n",
    "    print(\"Precision : \", tp/(tp + tn))\n",
    "    print(\"Recall: \", tp/(tp + fp))\n",
    "    print(\"Accuracy: \", (tp + fn)/(tp + tn + fp + fn))\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    mse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Linear SVC Results ------\n",
      "[[9395  163]\n",
      " [3664  337]]\n",
      "Precision :  0.9829462230592174\n",
      "Recall:  0.7194272149475458\n",
      "Accuracy:  0.717752046611107\n",
      "Mean Absolute Error (MAE): 0.28224795338889297\n",
      "Root Mean Squared Error (RMSE): 0.7288827973137014\n",
      "------ Exponential SVC Results ------\n",
      "[[9233  325]\n",
      " [3475  526]]\n",
      "Precision :  0.9659970705168446\n",
      "Recall:  0.7265502045955303\n",
      "Accuracy:  0.7197433439044177\n",
      "Mean Absolute Error (MAE): 0.28025665609558226\n",
      "Root Mean Squared Error (RMSE): 0.727593790434319\n"
     ]
    }
   ],
   "source": [
    "print(\"------ Polynomial SVC Results ------\")\n",
    "clf = make_pipeline(preprocessing.SplineTransformer(), SVC(kernel='poly'))\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------ Exponential SVC Results ------\")\n",
    "clf = make_pipeline(preprocessing.QuantileTransformer(), SVC(kernel='rbf'))\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "stats(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Boosting Results ------\n",
      "[[8846  638]\n",
      " [2935 1140]]\n",
      "Precision :  0.9327288064107971\n",
      "Recall:  0.750870044987692\n",
      "Accuracy:  0.7364849915185485\n",
      "Mean Absolute Error (MAE): 0.26351500848145143\n",
      "Root Mean Squared Error (RMSE): 0.7164754911783368\n",
      "------ Bagging Results ------\n",
      "[[8407 1077]\n",
      " [2688 1387]]\n",
      "Precision :  0.8864403205398566\n",
      "Recall:  0.7577287066246057\n",
      "Accuracy:  0.7223246552105612\n",
      "Mean Absolute Error (MAE): 0.2776753447894387\n",
      "Root Mean Squared Error (RMSE): 0.7259125920310261\n",
      "------ Stacking Results ------\n",
      "[[8687  797]\n",
      " [2607 1468]]\n",
      "Precision :  0.9159637283846478\n",
      "Recall:  0.7691694705153179\n",
      "Accuracy:  0.7489490375396416\n",
      "Mean Absolute Error (MAE): 0.2510509624603584\n",
      "Root Mean Squared Error (RMSE): 0.7078487552110309\n"
     ]
    }
   ],
   "source": [
    "print(\"------ Boosting Results ------\")\n",
    "model = make_pipeline(preprocessing.SplineTransformer() ,  GradientBoostingClassifier(learning_rate=0.1, max_depth=6, criterion='squared_error'))\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------ Bagging Results ------\")\n",
    "clf = BaggingClassifier(n_jobs=5)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------ Stacking Results ------\")\n",
    "neural = ('nn', make_pipeline(preprocessing.SplineTransformer(), MLPClassifier(hidden_layer_sizes=(16, 32), activation='relu', max_iter=200000)))\n",
    "clf = StackingClassifier(estimators=[\n",
    "    ('rf', RandomForestClassifier(max_depth=24, criterion='entropy', max_features='sqrt')), \n",
    "    ('svm', SVC(kernel='rbf')),\n",
    "    ('gb', GradientBoostingClassifier(max_depth=2))\n",
    "    ])\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "stats(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ KNN Results ------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m knn \u001b[39m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[39m=\u001b[39mk)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Train the kNN classifier\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m knn\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Make predictions on the test set\u001b[39;00m\n\u001b[1;32m      8\u001b[0m y_pred \u001b[39m=\u001b[39m knn\u001b[39m.\u001b[39mpredict(X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"------ KNN Results ------\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k = 2 \n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "# Train the kNN classifier\n",
    "knn.fit(X_train, y_train)\n",
    "# Make predictions on the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------ Decision Tree Results ------\")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(max_depth=20)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------ Random Forest Results ------\")\n",
    "clf = RandomForestClassifier(max_depth=24, criterion='entropy', max_features='sqrt')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "stats(y_pred, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.numeric_columns = ['DO_MGL','PH','temperature_2m (°C)', 'pressure_msl (hPa)', 'diffuse_radiation (W/m²)']\n",
    "        self.transformer = preprocessing.MaxAbsScaler()\n",
    "        transformed_data = self.transformer.fit_transform(dataframe[self.numeric_columns].values)\n",
    "        dataframe[self.numeric_columns] = transformed_data\n",
    "        self.data = dataframe.dropna(subset=['decreaseFeed']).reset_index(drop=True)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def getInverseTransform(self, inputs):\n",
    "        return self.quantile_transformer.inverse_transform(inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.data[self.numeric_columns].iloc[idx].values.astype(np.float32)\n",
    "        label = self.data['decreaseFeed'].iloc[idx].astype(np.float32)\n",
    "        inputs = torch.tensor(inputs)\n",
    "        label = torch.tensor(label)\n",
    "        return inputs, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artifical Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3, 16),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(16, 32),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(32, 8),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(8, 1),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reccurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extract_tensor(nn.Module):\n",
    "    def forward(self,x):\n",
    "        # Output shape (batch, features, hidden)\n",
    "        tensor, _ = x\n",
    "        # Reshape shape (batch, hidden)\n",
    "        return tensor[-1, :]\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.RNN(input_size=3, hidden_size=32, num_layers=5, batch_first=True, nonlinearity=\"relu\"),\n",
    "    extract_tensor(),\n",
    "    nn.CELU(),\n",
    "    nn.Linear(32, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.LSTM(input_size=3, hidden_size=16, num_layers=3 ,dropout=0.15),\n",
    "    extract_tensor(),\n",
    "    nn.Linear(16, 1)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mix of RNN and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.LSTM(input_size=3, hidden_size=8, num_layers=3 ,dropout=0.15),\n",
    "    extract_tensor(),\n",
    "    nn.Linear(8, 8),\n",
    "    nn.CELU(),\n",
    "    nn.RNN(input_size=8, hidden_size=16, num_layers=2, batch_first=True, nonlinearity=\"relu\"),\n",
    "    extract_tensor(),\n",
    "    nn.Linear(16, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "  (1): Dropout(p=0.5, inplace=False)\n",
      "  (2): LeakyReLU(negative_slope=0.01)\n",
      "  (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (4): Dropout(p=0.5, inplace=False)\n",
      "  (5): LeakyReLU(negative_slope=0.01)\n",
      "  (6): Linear(in_features=32, out_features=8, bias=True)\n",
      "  (7): Dropout(p=0.5, inplace=False)\n",
      "  (8): LeakyReLU(negative_slope=0.01)\n",
      "  (9): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n",
      "Epoch 1, Training Loss: 0.1157\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.SmoothL1Loss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "num_epochs = 1\n",
    "\n",
    "test = comb.copy(True)\n",
    "\n",
    "# split testset into training and tetsing\n",
    "trainset, testset = train_test_split(test, test_size=0.15)\n",
    "\n",
    "dataset = MyDataset(trainset)\n",
    "data_loader = DataLoader(dataset, drop_last=True)\n",
    "\n",
    "print(model)\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for inputs, labels in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        labels = labels.view(outputs.shape)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += abs(loss.item())\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9545    1]\n",
      " [4013    0]]\n",
      "Precision :  0.9998952440812906\n",
      "Recall:  0.7040123912081427\n",
      "Accuracy:  0.7039604690611402\n",
      "Mean Absolute Error (MAE): 0.2960395309388598\n",
      "Root Mean Squared Error (RMSE): 0.7376280558600531\n"
     ]
    }
   ],
   "source": [
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "dataset = MyDataset(testset)\n",
    "data_loader = DataLoader(dataset)\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    for inputs, label in data_loader:\n",
    "        # Forward pass to get predictions\n",
    "        y_pred = model(inputs.unsqueeze(1))\n",
    "        if y_pred <= 0.5:\n",
    "            y_pred_all.append(0)  # Append zero to the list\n",
    "        else:\n",
    "            y_pred_all.append(1)  # Append one to the list\n",
    "        \n",
    "        # Convert the predictions and targets to numpy arrays\n",
    "        label_np = label.numpy()\n",
    "\n",
    "        # Append batch results to the overall lists\n",
    "        y_test_all.append(label_np)\n",
    "\n",
    "# Concatenate the lists into a single array\n",
    "y_test_all = np.array(y_test_all)\n",
    "y_pred_all = np.array(y_pred_all)\n",
    "\n",
    "stats(y_pred_all, y_test_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"../datasets/Philly/DO_QAQC.csv\")\n",
    "\n",
    "# Check if DateTime_EST exists in the DataFrame\n",
    "if 'DateTime_EST' in data.columns:\n",
    "    # Convert DateTime_EST to datetime\n",
    "    data['DateTime_EST'] = pd.to_datetime(data['DateTime_EST'])\n",
    "else:\n",
    "    raise ValueError(\"DateTime_EST column doesn't exist in the provided dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['U_A_0', 'U_A_2.5', 'D_A_4', 'D_A_5.5', 'U_B_12', 'D_B_14.5',\n",
       "       'D_B_15.5', 'D_All_16.5', 'D_All_33.5'], dtype=object)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Site'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = pd.read_csv(\"../datasets/Philly/phillyweather.csv\")\n",
    "weather_data = weather_data.rename(columns={'time': 'DateTime_EST'})\n",
    "weather_data['DateTime_EST'] = pd.to_datetime(weather_data['DateTime_EST'])\n",
    "\n",
    "# Merge the datasets on 'DateTime_EST'\n",
    "merged_data = pd.merge(data, weather_data, on='DateTime_EST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target\n",
    "features = merged_data[['temperature_2m (°C)', 'relativehumidity_2m (%)', 'surface_pressure (hPa)', \n",
    "                        'windspeed_10m (km/h)', 'direct_radiation (W/m²)', 'diffuse_radiation (W/m²)', 'Depth_m']]\n",
    "target = merged_data['DO_mg_L']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2948925792455934, 1.9282660533707225, 2.512321967703182)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exclude records with missing target values from the features and target datasets\n",
    "missing_target_index = y_train.index[y_train.isnull()]\n",
    "X_train = X_train.drop(missing_target_index)\n",
    "y_train = y_train.dropna()\n",
    "\n",
    "# Normalize the features using StandardScaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Train the SVR model\n",
    "svr.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = svr.predict(X_test_pca)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print (\"SVR on whole dataset\")\n",
    "r2, mae, rmse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.481404427479976, 1.200479630566635, 1.6595924432940934)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the SVR model\n",
    "svr = SVR(kernel='rbf')\n",
    "\n",
    "# Initialize the StandardScaler and PCA\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=0.95, svd_solver='full')  # retain 95% of the variance\n",
    "\n",
    "# Select the subset of data from site 'D_A_5.5'\n",
    "data_subset = merged_data[merged_data['Site'] == 'D_A_5.5']\n",
    "features_subset = data_subset[['temperature_2m (°C)', 'relativehumidity_2m (%)', 'surface_pressure (hPa)', \n",
    "                                  'windspeed_10m (km/h)', 'direct_radiation (W/m²)', 'diffuse_radiation (W/m²)', 'Depth_m']]\n",
    "target_subset = data_subset['DO_mg_L']\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_subset, X_test_subset, y_train_subset, y_test_subset = train_test_split(features_subset, target_subset, \n",
    "                                                                                test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using StandardScaler\n",
    "X_train_scaled_subset = scaler.fit_transform(X_train_subset)\n",
    "X_test_scaled_subset = scaler.transform(X_test_subset)\n",
    "\n",
    "# Apply PCA\n",
    "X_train_pca_subset = pca.fit_transform(X_train_scaled_subset)\n",
    "X_test_pca_subset = pca.transform(X_test_scaled_subset)\n",
    "\n",
    "# Train the SVR model and compute evaluation metrics\n",
    "svr.fit(X_train_pca_subset, y_train_subset)\n",
    "y_pred_subset = svr.predict(X_test_pca_subset)\n",
    "\n",
    "r2 = r2_score(y_test_subset, y_pred_subset)\n",
    "mae = mean_absolute_error(y_test_subset, y_pred_subset)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_subset, y_pred_subset))\n",
    "\n",
    "r2, mae, rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46546"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest, Gradient boosting, and LightGMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest on subset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gradient Boosting on subset...\n",
      "Training XGBoost on subset...\n",
      "Training LightGBM on subset...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Initialize the models\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42),\n",
    "    'LightGBM': LGBMRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "data_subset = merged_data[merged_data['Site'] == 'D_A_5.5']\n",
    "features_subset = data_subset[['temperature_2m (°C)', 'relativehumidity_2m (%)', 'surface_pressure (hPa)', \n",
    "                                  'windspeed_10m (km/h)', 'direct_radiation (W/m²)', 'diffuse_radiation (W/m²)', 'Depth_m']]\n",
    "target_subset = data_subset['DO_mg_L']\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_subset, X_test_subset, y_train_subset, y_test_subset = train_test_split(features_subset, target_subset, \n",
    "                                                                                test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using StandardScaler\n",
    "X_train_scaled_subset = scaler.fit_transform(X_train_subset)\n",
    "X_test_scaled_subset = scaler.transform(X_test_subset)\n",
    "\n",
    "# Apply PCA\n",
    "X_train_pca_subset = pca.fit_transform(X_train_scaled_subset)\n",
    "X_test_pca_subset = pca.transform(X_test_scaled_subset)\n",
    "\n",
    "\n",
    "# Function to train a model and compute evaluation metrics\n",
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    return r2, mae, rmse\n",
    "\n",
    "# Results will be stored in this dictionary\n",
    "results = {}\n",
    "\n",
    "# Train each model on the whole dataset\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name} on subset...\")\n",
    "    r2, mae, rmse = train_and_evaluate(model, X_train_pca_subset, X_test_pca_subset, y_train_subset, y_test_subset)\n",
    "    results[(model_name, 'Whole dataset')] = {'R^2': r2, 'MAE': mae, 'RMSE': rmse}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Random Forest', 'Whole dataset'): {'R^2': 0.616813397445579,\n",
       "  'MAE': 1.3651664193232371,\n",
       "  'RMSE': 1.8520504440251297},\n",
       " ('Gradient Boosting', 'Whole dataset'): {'R^2': 0.40270455047862164,\n",
       "  'MAE': 1.7554510928439677,\n",
       "  'RMSE': 2.3122897615839766},\n",
       " ('XGBoost', 'Whole dataset'): {'R^2': 0.5224408663405107,\n",
       "  'MAE': 1.5618134436798894,\n",
       "  'RMSE': 2.0675748034771746},\n",
       " ('LightGBM', 'Whole dataset'): {'R^2': 0.49698106610811,\n",
       "  'MAE': 1.606663644924342,\n",
       "  'RMSE': 2.121972832598724}}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results #whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Random Forest', 'Whole dataset'): {'R^2': 0.6295954638212846,\n",
       "  'MAE': 1.1536115570934258,\n",
       "  'RMSE': 1.5160071120086804},\n",
       " ('Gradient Boosting', 'Whole dataset'): {'R^2': 0.48036858619119827,\n",
       "  'MAE': 1.401986339871773,\n",
       "  'RMSE': 1.7956049310925377},\n",
       " ('XGBoost', 'Whole dataset'): {'R^2': 0.5666777365815845,\n",
       "  'MAE': 1.26670609478802,\n",
       "  'RMSE': 1.6397158392227416},\n",
       " ('LightGBM', 'Whole dataset'): {'R^2': 0.5854989989644558,\n",
       "  'MAE': 1.2453077226106546,\n",
       "  'RMSE': 1.6037101660199813}}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

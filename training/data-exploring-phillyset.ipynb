{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"../datasets/Philly/DO_QAQC.csv\")\n",
    "\n",
    "# Check if DateTime_EST exists in the DataFrame\n",
    "if 'DateTime_EST' in data.columns:\n",
    "    # Convert DateTime_EST to datetime\n",
    "    data['DateTime_EST'] = pd.to_datetime(data['DateTime_EST'])\n",
    "else:\n",
    "    raise ValueError(\"DateTime_EST column doesn't exist in the provided dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['U_A_0', 'U_A_2.5', 'D_A_4', 'D_A_5.5', 'U_B_12', 'D_B_14.5',\n",
       "       'D_B_15.5', 'D_All_16.5', 'D_All_33.5'], dtype=object)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Site'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = pd.read_csv(\"../datasets/Philly/phillyweather.csv\")\n",
    "weather_data = weather_data.rename(columns={'time': 'DateTime_EST'})\n",
    "weather_data['DateTime_EST'] = pd.to_datetime(weather_data['DateTime_EST'])\n",
    "\n",
    "# Merge the datasets on 'DateTime_EST'\n",
    "merged_data = pd.merge(data, weather_data, on='DateTime_EST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target\n",
    "features = merged_data[['temperature_2m (°C)', 'relativehumidity_2m (%)', 'surface_pressure (hPa)', \n",
    "                        'windspeed_10m (km/h)', 'direct_radiation (W/m²)', 'diffuse_radiation (W/m²)', 'Depth_m']]\n",
    "target = merged_data['DO_mg_L']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2948925792455934, 1.9282660533707225, 2.512321967703182)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exclude records with missing target values from the features and target datasets\n",
    "missing_target_index = y_train.index[y_train.isnull()]\n",
    "X_train = X_train.drop(missing_target_index)\n",
    "y_train = y_train.dropna()\n",
    "\n",
    "# Normalize the features using StandardScaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Train the SVR model\n",
    "svr.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = svr.predict(X_test_pca)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print (\"SVR on whole dataset\")\n",
    "r2, mae, rmse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.481404427479976, 1.200479630566635, 1.6595924432940934)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the SVR model\n",
    "svr = SVR(kernel='rbf')\n",
    "\n",
    "# Initialize the StandardScaler and PCA\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=0.95, svd_solver='full')  # retain 95% of the variance\n",
    "\n",
    "# Select the subset of data from site 'D_A_5.5'\n",
    "data_subset = merged_data[merged_data['Site'] == 'D_A_5.5']\n",
    "features_subset = data_subset[['temperature_2m (°C)', 'relativehumidity_2m (%)', 'surface_pressure (hPa)', \n",
    "                                  'windspeed_10m (km/h)', 'direct_radiation (W/m²)', 'diffuse_radiation (W/m²)', 'Depth_m']]\n",
    "target_subset = data_subset['DO_mg_L']\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_subset, X_test_subset, y_train_subset, y_test_subset = train_test_split(features_subset, target_subset, \n",
    "                                                                                test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using StandardScaler\n",
    "X_train_scaled_subset = scaler.fit_transform(X_train_subset)\n",
    "X_test_scaled_subset = scaler.transform(X_test_subset)\n",
    "\n",
    "# Apply PCA\n",
    "X_train_pca_subset = pca.fit_transform(X_train_scaled_subset)\n",
    "X_test_pca_subset = pca.transform(X_test_scaled_subset)\n",
    "\n",
    "# Train the SVR model and compute evaluation metrics\n",
    "svr.fit(X_train_pca_subset, y_train_subset)\n",
    "y_pred_subset = svr.predict(X_test_pca_subset)\n",
    "\n",
    "r2 = r2_score(y_test_subset, y_pred_subset)\n",
    "mae = mean_absolute_error(y_test_subset, y_pred_subset)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_subset, y_pred_subset))\n",
    "\n",
    "r2, mae, rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46546"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest, Gradient boosting, and LightGMB"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest on subset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gradient Boosting on subset...\n",
      "Training XGBoost on subset...\n",
      "Training LightGBM on subset...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phil = pd.read_csv(\"../datasets/Philly/DO_QAQC.csv\")\n",
    "phil['DateTime_EST'] = pd.to_datetime(phil['DateTime_EST'])\n",
    "phil = phil.rename(columns={'DateTime_EST': 'time'}, inplace=False)\n",
    "phil = phil[phil['Site'] == 'U_A_0']\n",
    "print(len(phil))\n",
>>>>>>> ee9387f11ee4d9a2ec569aa117887c7a86841601
    "\n",
    "# Initialize the models\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42),\n",
    "    'LightGBM': LGBMRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "data_subset = merged_data[merged_data['Site'] == 'D_A_5.5']\n",
    "features_subset = data_subset[['temperature_2m (°C)', 'relativehumidity_2m (%)', 'surface_pressure (hPa)', \n",
    "                                  'windspeed_10m (km/h)', 'direct_radiation (W/m²)', 'diffuse_radiation (W/m²)', 'Depth_m']]\n",
    "target_subset = data_subset['DO_mg_L']\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_subset, X_test_subset, y_train_subset, y_test_subset = train_test_split(features_subset, target_subset, \n",
    "                                                                                test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using StandardScaler\n",
    "X_train_scaled_subset = scaler.fit_transform(X_train_subset)\n",
    "X_test_scaled_subset = scaler.transform(X_test_subset)\n",
    "\n",
    "# Apply PCA\n",
    "X_train_pca_subset = pca.fit_transform(X_train_scaled_subset)\n",
    "X_test_pca_subset = pca.transform(X_test_scaled_subset)\n",
    "\n",
    "\n",
    "# Function to train a model and compute evaluation metrics\n",
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    return r2, mae, rmse\n",
    "\n",
    "# Results will be stored in this dictionary\n",
    "results = {}\n",
    "\n",
    "# Train each model on the whole dataset\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name} on subset...\")\n",
    "    r2, mae, rmse = train_and_evaluate(model, X_train_pca_subset, X_test_pca_subset, y_train_subset, y_test_subset)\n",
    "    results[(model_name, 'Whole dataset')] = {'R^2': r2, 'MAE': mae, 'RMSE': rmse}\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Random Forest', 'Whole dataset'): {'R^2': 0.616813397445579,\n",
       "  'MAE': 1.3651664193232371,\n",
       "  'RMSE': 1.8520504440251297},\n",
       " ('Gradient Boosting', 'Whole dataset'): {'R^2': 0.40270455047862164,\n",
       "  'MAE': 1.7554510928439677,\n",
       "  'RMSE': 2.3122897615839766},\n",
       " ('XGBoost', 'Whole dataset'): {'R^2': 0.5224408663405107,\n",
       "  'MAE': 1.5618134436798894,\n",
       "  'RMSE': 2.0675748034771746},\n",
       " ('LightGBM', 'Whole dataset'): {'R^2': 0.49698106610811,\n",
       "  'MAE': 1.606663644924342,\n",
       "  'RMSE': 2.121972832598724}}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column in philweather called roundedNearestHour and input weather data\n",
    "# time,temperature_2m (°C),relativehumidity_2m (%),precipitation (mm),surface_pressure (hPa),windspeed_10m (km/h),direct_radiation (W/m²),diffuse_radiation (W/m²)\n",
    "phil['roundedNearestHour'] = phil['time'].dt.round('H')\n",
    "phil['temperature_2m (°C)'] = phil['roundedNearestHour'].map(philweather.set_index('time')['temperature_2m (°C)'])\n",
    "phil['relativehumidity_2m (%)'] = phil['roundedNearestHour'].map(philweather.set_index('time')['relativehumidity_2m (%)'])\n",
    "phil['precipitation (mm)'] = phil['roundedNearestHour'].map(philweather.set_index('time')['precipitation (mm)'])\n",
    "phil['surface_pressure (hPa)'] = phil['roundedNearestHour'].map(philweather.set_index('time')['surface_pressure (hPa)'])\n",
    "phil['windspeed_10m (km/h)'] = phil['roundedNearestHour'].map(philweather.set_index('time')['windspeed_10m (km/h)'])\n",
    "phil['direct_radiation (W/m²)'] = phil['roundedNearestHour'].map(philweather.set_index('time')['direct_radiation (W/m²)'])\n",
    "phil['diffuse_radiation (W/m²)'] = phil['roundedNearestHour'].map(philweather.set_index('time')['diffuse_radiation (W/m²)'])\n",
    "\n",
    "\n",
    "phil.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phil.rename(columns={\n",
    "    'Temp_deg_C': 'temperature',\n",
    "    'temperature_2m (°C)': 'airtemp',\n",
    "    'Depth_m': 'Depth',\n",
    "    'diffuse_radiation (W/m²)': 'light'\n",
    "}, inplace=True)\n",
    "phil.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phil['Depth'] = 1.2192\n",
    "phil['temperature'] = pd.to_numeric(phil['temperature'])\n",
    "phil['temperature^2'] = phil['temperature'] * phil['temperature']\n",
    "phil['airtemp^2'] = phil['airtemp'] * phil['airtemp']\n",
    "phil['temp*airtemp'] = phil['temperature'] * phil['airtemp']\n",
    "phil['depth*temp'] = phil['Depth'] * phil['temperature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> ee9387f11ee4d9a2ec569aa117887c7a86841601
   "source": [
    "results #whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Random Forest', 'Whole dataset'): {'R^2': 0.6295954638212846,\n",
       "  'MAE': 1.1536115570934258,\n",
       "  'RMSE': 1.5160071120086804},\n",
       " ('Gradient Boosting', 'Whole dataset'): {'R^2': 0.48036858619119827,\n",
       "  'MAE': 1.401986339871773,\n",
       "  'RMSE': 1.7956049310925377},\n",
       " ('XGBoost', 'Whole dataset'): {'R^2': 0.5666777365815845,\n",
       "  'MAE': 1.26670609478802,\n",
       "  'RMSE': 1.6397158392227416},\n",
       " ('LightGBM', 'Whole dataset'): {'R^2': 0.5854989989644558,\n",
       "  'MAE': 1.2453077226106546,\n",
       "  'RMSE': 1.6037101660199813}}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": []
=======
   "source": [
    "'''\n",
    "nf.rename(columns={\n",
    "    'Temp_deg_C^2': 'temperature^2',\n",
    "    'temperature_2m^2': 'airtemp^2',\n",
    "    'Temp_2m_interaction': 'temp*airtemp',\n",
    "    'Depth_Temp_interaction': 'depth*temp',\n",
    "    'Depth_m': 'Depth',\n",
    "    'windspeed_10m (km/h)': 'windspeed_10m (km/h)',\n",
    "    'diffuse_radiation': 'light'\n",
    "}, inplace=True)\n",
    "'''\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    phil[['temperature^2', 'airtemp^2', 'temp*airtemp', 'depth*temp', 'Depth', 'windspeed_10m (km/h)', 'light']],\n",
    "    phil['DO_mg_L'],\n",
    "    test_size=0.2\n",
    ")\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import math\n",
    "\n",
    "def stats(y_pred_all, y_test_all):\n",
    "    # Calculate the R2 score\n",
    "    r2 = r2_score(y_test_all, y_pred_all)\n",
    "\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y_test_all, y_pred_all)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    mse = mean_squared_error(y_test_all, y_pred_all, squared=False)\n",
    "\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------ Random Forest Results ------------\")\n",
    "rf = RandomForestRegressor(n_estimators=24, max_depth=40, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "print(rf.feature_importances_)\n",
    "y_pred = rf.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------------ Decision Tree Results ------------\")\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "d_tree = DecisionTreeRegressor(max_depth=24)\n",
    "d_tree.fit(X_train, y_train)\n",
    "y_pred = d_tree.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------------ XG Boost Results ------------\")\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "xg_boost = GradientBoostingRegressor(learning_rate=0.1, loss='huber', max_depth=6, criterion='squared_error')\n",
    "xg_boost.fit(X_train, y_train)\n",
    "y_pred = xg_boost.predict(X_test)\n",
    "stats(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------ Poly Regression Results ------------\")\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=12, include_bias=True)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "# Initialize and fit the linear regression model\n",
    "poly_reg = LinearRegression()\n",
    "poly_reg.fit(X_train_poly, y_train)\n",
    "# Predict the target variable for training and test sets\n",
    "y_pred = poly_reg.predict(X_test_poly)\n",
    "stats(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------ Exponential SVR Results ------------\")\n",
    "clf_rbf_nusvm = make_pipeline(preprocessing.SplineTransformer(), NuSVR(kernel='rbf', shrinking=True, C=1.5))\n",
    "clf_rbf_nusvm.fit(X_train, y_train)\n",
    "y_pred = clf_rbf_nusvm.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------------ Exponential SVR Results ------------\")\n",
    "clf_rbf_svm = make_pipeline(preprocessing.SplineTransformer(), SVR(kernel='rbf', shrinking=True, C=1.5))\n",
    "clf_rbf_svm.fit(X_train[:len(X_train)//2], y_train[:len(X_train)//2])\n",
    "clf_rbf_svm.fit(X_train[len(X_train)//2:], y_train[len(X_train)//2:])\n",
    "y_pred = clf_rbf_svm.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------------ Polynomial SVR Results ------------\")\n",
    "clf_poly_nusvm = make_pipeline(preprocessing.SplineTransformer(), NuSVR(kernel='poly', shrinking=False, C=2.5))\n",
    "clf_poly_nusvm.fit(X_train, y_train)\n",
    "y_pred = clf_poly_nusvm.predict(X_test)\n",
    "stats(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------ Bagging Results ------------\")\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "base_estimator = DecisionTreeRegressor()\n",
    "bag_pipe = make_pipeline(preprocessing.SplineTransformer(), BaggingRegressor(base_estimator=base_estimator, n_estimators=10, n_jobs=5))\n",
    "bag_pipe.fit(X_train, y_train)\n",
    "y_pred = bag_pipe.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------------ Extra Trees Results ------------\")\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "extra_pipe = make_pipeline(preprocessing.SplineTransformer(), ExtraTreesRegressor(n_estimators=10, n_jobs=5))\n",
    "extra_pipe.fit(X_train, y_train)\n",
    "y_pred = extra_pipe.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------------ Ada Boost Results ------------\")\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "ada_pipe = make_pipeline(preprocessing.SplineTransformer(), AdaBoostRegressor(base_estimator=base_estimator, n_estimators=10))\n",
    "ada_pipe.fit(X_train, y_train)\n",
    "y_pred = ada_pipe.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "'''\n",
    "print(\"------------ Voting Results ------------\")\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "voting_pipe = make_pipeline(preprocessing.SplineTransformer(), VotingRegressor(estimators=[\n",
    "    ('svr', NuSVR(kernel='poly', shrinking=False, C=2.5)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=10,random_state=42, n_jobs=8)),\n",
    "    ('bag', BaggingRegressor(n_jobs=5)),\n",
    "    ('bst', GradientBoostingRegressor(learning_rate=0.1, loss='huber', max_depth=6, criterion='squared_error')),\n",
    "    ('n1', MLPRegressor(hidden_layer_sizes=(2,3), activation='relu')),  \n",
    "    ('n2', MLPRegressor(hidden_layer_sizes=(3,2), activation='tanh'))\n",
    "]))\n",
    "voting_pipe.fit(X_train, y_train)\n",
    "y_pred = voting_pipe.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "\n",
    "print(\"------------ Stacking Results ------------\")\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "model = StackingRegressor(estimators=[\n",
    "    ('svr', NuSVR(kernel='poly', shrinking=False, C=2.5)), \n",
    "    ('rf', RandomForestRegressor(n_estimators=10,random_state=42, n_jobs=8)),\n",
    "    ('bag', BaggingRegressor(n_jobs=5)),\n",
    "    ('bst', GradientBoostingRegressor(learning_rate=0.1, loss='huber', max_depth=6, criterion='squared_error')),\n",
    "    ('n1', MLPRegressor(hidden_layer_sizes=(2,3), activation='relu')),\n",
    "    ('n2', MLPRegressor(hidden_layer_sizes=(3,2), activation='tanh'))\n",
    "])\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "# Connect to the database\n",
    "connection = pymysql.connect(\n",
    "    host='localhost', \n",
    "    user='root', \n",
    "    password='N@wid2003', \n",
    "    db='dma_iot_morefish_spark_farms_v3'\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Get the latest data from the database and store it in a pandas dataframe\n",
    "query = \"SELECT dvd_ph, dvd_temp, dvd_updated_at, dvd_do, dvd_dev_id FROM device_devicedata WHERE dvd_ph > 0 AND dvd_temp > 0 AND dvd_dev_id = 2 ORDER BY dvd_updated_at ASC\"\n",
    "df = pd.read_sql(query, connection)\n",
    "\n",
    "# Close the connection\n",
    "connection.close()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "excelData = pd.read_excel(\"../../../../../../Downloads/device_data_july13.xlsx\")\n",
    "excelData.to_csv(\"../datasets/Philly/device_data_july13.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'dvd_ph': 'ph', 'dvd_temp': 'temperature', 'dvd_updated_at': 'datetime', 'dvd_do': 'do_linreg'}, inplace=True)\n",
    "df['rounded_datetime'] = pd.to_datetime(df['datetime']).dt.round('H')\n",
    "# spark_weather = pd.read_csv(\"../../../../../../Downloads/sparkfarmsweather.csv\")\n",
    "spark_weather = pd.read_csv(\"../datasets/sparkfarmsweather.csv\")\n",
    "spark_weather['time'] = pd.to_datetime(spark_weather['time'])\n",
    "spark_weather.rename(columns={'time': 'datetime'}, inplace=True)\n",
    "# Merge the weather data with the main DataFrame based on 'rounded_datetime' column\n",
    "df = df.merge(spark_weather, left_on='rounded_datetime', right_on='datetime', how='left')\n",
    "df = df.drop(columns=['datetime_y'])\n",
    "df.rename(columns={'datetime_x': 'datetime', 'diffuse_radiation (W/m²)': 'light', 'relativehumidity_2m (%)': 'humid', 'temperature_2m (°C)': 'airtemp' }, inplace=True)\n",
    "# drop all NaN values inside diffuse radiation\n",
    "df = df.dropna(subset=['light'])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new features: depth, temperature^2, airtemp^2, temp*airtemp, depth*temp\n",
    "\n",
    "df['Depth'] = 1.2192\n",
    "df['temperature'] = pd.to_numeric(df['temperature'])\n",
    "df['temperature^2'] = df['temperature'] * df['temperature']\n",
    "df['airtemp^2'] = df['airtemp'] * df['airtemp']\n",
    "df['temp*airtemp'] = df['temperature'] * df['airtemp']\n",
    "df['depth*temp'] = df['Depth'] * df['temperature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_test = df[['temperature^2', 'airtemp^2','temp*airtemp', 'depth*temp', 'Depth', 'windspeed_10m (km/h)', 'light']]\n",
    "y_pred = poly_reg.predict(poly_features.transform(sf_test))\n",
    "df['do_poly'] = y_pred\n",
    "df[['do_linreg', 'do_poly']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "myDates = [datetime(2012,1,i+3) for i in range(10)]\n",
    "myValues = [5,6,4,3,7,8,1,2,5,4]\n",
    "fig, ax = plt.subplots()\n",
    "bet = df.loc[df['datetime'].between('2023-06-10', '2023-06-13')]\n",
    "ax.plot(bet['datetime'], bet['do_poly'].astype(float), 'bo')\n",
    "\n",
    "myFmt = DateFormatter(\"%m %d %H:%M\")\n",
    "ax.xaxis.set_major_formatter(myFmt)\n",
    "\n",
    "## Rotate date labels automatically\n",
    "fig.autofmt_xdate()\n",
    "plt.show()"
   ]
>>>>>>> ee9387f11ee4d9a2ec569aa117887c7a86841601
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

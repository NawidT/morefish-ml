{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.feature_selection as fs\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.svm import SVR, NuSVR\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phil = pd.read_csv(\"../datasets/Philly/DO_QAQC.csv\")\n",
    "phil['DateTime_EST'] = pd.to_datetime(phil['DateTime_EST'])\n",
    "phil = phil.rename(columns={'DateTime_EST': 'time'}, inplace=False)\n",
    "phil = phil[phil['Site'] == 'U_A_0']\n",
    "print(len(phil))\n",
    "\n",
    "philweather = pd.read_csv(\"../datasets/Philly/phillyweather.csv\")\n",
    "philweather['time'] = pd.to_datetime(philweather['time'])\n",
    "print(len(philweather))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column in philweather called roundedNearestHour and input weather data\n",
    "# time,temperature_2m (°C),relativehumidity_2m (%),precipitation (mm),surface_pressure (hPa),windspeed_10m (km/h),direct_radiation (W/m²),diffuse_radiation (W/m²)\n",
    "phil['roundedNearestHour'] = phil['time'].dt.round('H')\n",
    "phil['temperature_2m (°C)'] = phil['roundedNearestHour'].map(philweather.set_index('time')['temperature_2m (°C)'])\n",
    "phil['relativehumidity_2m (%)'] = phil['roundedNearestHour'].map(philweather.set_index('time')['relativehumidity_2m (%)'])\n",
    "phil['precipitation (mm)'] = phil['roundedNearestHour'].map(philweather.set_index('time')['precipitation (mm)'])\n",
    "phil['surface_pressure (hPa)'] = phil['roundedNearestHour'].map(philweather.set_index('time')['surface_pressure (hPa)'])\n",
    "phil['windspeed_10m (km/h)'] = phil['roundedNearestHour'].map(philweather.set_index('time')['windspeed_10m (km/h)'])\n",
    "phil['direct_radiation (W/m²)'] = phil['roundedNearestHour'].map(philweather.set_index('time')['direct_radiation (W/m²)'])\n",
    "phil['diffuse_radiation (W/m²)'] = phil['roundedNearestHour'].map(philweather.set_index('time')['diffuse_radiation (W/m²)'])\n",
    "\n",
    "\n",
    "phil.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phil.rename(columns={\n",
    "    'Temp_deg_C': 'temperature',\n",
    "    'temperature_2m (°C)': 'airtemp',\n",
    "    'Depth_m': 'Depth',\n",
    "    'diffuse_radiation (W/m²)': 'light'\n",
    "}, inplace=True)\n",
    "phil.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phil['Depth'] = 1.2192\n",
    "phil['temperature'] = pd.to_numeric(phil['temperature'])\n",
    "phil['temperature^2'] = phil['temperature'] * phil['temperature']\n",
    "phil['airtemp^2'] = phil['airtemp'] * phil['airtemp']\n",
    "phil['temp*airtemp'] = phil['temperature'] * phil['airtemp']\n",
    "phil['depth*temp'] = phil['Depth'] * phil['temperature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf =  pd.read_csv(\"../datasets/Philly/new_features.csv\")\n",
    "nf['time'] = phil['time']\n",
    "\n",
    "nf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "nf.rename(columns={\n",
    "    'Temp_deg_C^2': 'temperature^2',\n",
    "    'temperature_2m^2': 'airtemp^2',\n",
    "    'Temp_2m_interaction': 'temp*airtemp',\n",
    "    'Depth_Temp_interaction': 'depth*temp',\n",
    "    'Depth_m': 'Depth',\n",
    "    'windspeed_10m (km/h)': 'windspeed_10m (km/h)',\n",
    "    'diffuse_radiation': 'light'\n",
    "}, inplace=True)\n",
    "'''\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    phil[['temperature^2', 'airtemp^2', 'temp*airtemp', 'depth*temp', 'Depth', 'windspeed_10m (km/h)', 'light']],\n",
    "    phil['DO_mg_L'],\n",
    "    test_size=0.2\n",
    ")\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import math\n",
    "\n",
    "def stats(y_pred_all, y_test_all):\n",
    "    # Calculate the R2 score\n",
    "    r2 = r2_score(y_test_all, y_pred_all)\n",
    "\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y_test_all, y_pred_all)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    mse = mean_squared_error(y_test_all, y_pred_all, squared=False)\n",
    "\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------ Random Forest Results ------------\")\n",
    "rf = RandomForestRegressor(n_estimators=24, max_depth=40, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "print(rf.feature_importances_)\n",
    "y_pred = rf.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------------ Decision Tree Results ------------\")\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "d_tree = DecisionTreeRegressor(max_depth=24)\n",
    "d_tree.fit(X_train, y_train)\n",
    "y_pred = d_tree.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------------ XG Boost Results ------------\")\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "xg_boost = GradientBoostingRegressor(learning_rate=0.1, loss='huber', max_depth=6, criterion='squared_error')\n",
    "xg_boost.fit(X_train, y_train)\n",
    "y_pred = xg_boost.predict(X_test)\n",
    "stats(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------ Poly Regression Results ------------\")\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=12, include_bias=True)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "# Initialize and fit the linear regression model\n",
    "poly_reg = LinearRegression()\n",
    "poly_reg.fit(X_train_poly, y_train)\n",
    "# Predict the target variable for training and test sets\n",
    "y_pred = poly_reg.predict(X_test_poly)\n",
    "stats(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------ Exponential SVR Results ------------\")\n",
    "clf_rbf_nusvm = make_pipeline(preprocessing.SplineTransformer(), NuSVR(kernel='rbf', shrinking=True, C=1.5))\n",
    "clf_rbf_nusvm.fit(X_train, y_train)\n",
    "y_pred = clf_rbf_nusvm.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------------ Exponential SVR Results ------------\")\n",
    "clf_rbf_svm = make_pipeline(preprocessing.SplineTransformer(), SVR(kernel='rbf', shrinking=True, C=1.5))\n",
    "clf_rbf_svm.fit(X_train[:len(X_train)//2], y_train[:len(X_train)//2])\n",
    "clf_rbf_svm.fit(X_train[len(X_train)//2:], y_train[len(X_train)//2:])\n",
    "y_pred = clf_rbf_svm.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------------ Polynomial SVR Results ------------\")\n",
    "clf_poly_nusvm = make_pipeline(preprocessing.SplineTransformer(), NuSVR(kernel='poly', shrinking=False, C=2.5))\n",
    "clf_poly_nusvm.fit(X_train, y_train)\n",
    "y_pred = clf_poly_nusvm.predict(X_test)\n",
    "stats(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------ Bagging Results ------------\")\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "base_estimator = DecisionTreeRegressor()\n",
    "bag_pipe = make_pipeline(preprocessing.SplineTransformer(), BaggingRegressor(base_estimator=base_estimator, n_estimators=10, n_jobs=5))\n",
    "bag_pipe.fit(X_train, y_train)\n",
    "y_pred = bag_pipe.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------------ Extra Trees Results ------------\")\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "extra_pipe = make_pipeline(preprocessing.SplineTransformer(), ExtraTreesRegressor(n_estimators=10, n_jobs=5))\n",
    "extra_pipe.fit(X_train, y_train)\n",
    "y_pred = extra_pipe.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "print(\"------------ Ada Boost Results ------------\")\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "ada_pipe = make_pipeline(preprocessing.SplineTransformer(), AdaBoostRegressor(base_estimator=base_estimator, n_estimators=10))\n",
    "ada_pipe.fit(X_train, y_train)\n",
    "y_pred = ada_pipe.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "'''\n",
    "print(\"------------ Voting Results ------------\")\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "voting_pipe = make_pipeline(preprocessing.SplineTransformer(), VotingRegressor(estimators=[\n",
    "    ('svr', NuSVR(kernel='poly', shrinking=False, C=2.5)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=10,random_state=42, n_jobs=8)),\n",
    "    ('bag', BaggingRegressor(n_jobs=5)),\n",
    "    ('bst', GradientBoostingRegressor(learning_rate=0.1, loss='huber', max_depth=6, criterion='squared_error')),\n",
    "    ('n1', MLPRegressor(hidden_layer_sizes=(2,3), activation='relu')),  \n",
    "    ('n2', MLPRegressor(hidden_layer_sizes=(3,2), activation='tanh'))\n",
    "]))\n",
    "voting_pipe.fit(X_train, y_train)\n",
    "y_pred = voting_pipe.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "\n",
    "\n",
    "print(\"------------ Stacking Results ------------\")\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "model = StackingRegressor(estimators=[\n",
    "    ('svr', NuSVR(kernel='poly', shrinking=False, C=2.5)), \n",
    "    ('rf', RandomForestRegressor(n_estimators=10,random_state=42, n_jobs=8)),\n",
    "    ('bag', BaggingRegressor(n_jobs=5)),\n",
    "    ('bst', GradientBoostingRegressor(learning_rate=0.1, loss='huber', max_depth=6, criterion='squared_error')),\n",
    "    ('n1', MLPRegressor(hidden_layer_sizes=(2,3), activation='relu')),\n",
    "    ('n2', MLPRegressor(hidden_layer_sizes=(3,2), activation='tanh'))\n",
    "])\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "stats(y_pred, y_test)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "# Connect to the database\n",
    "connection = pymysql.connect(\n",
    "    host='localhost', \n",
    "    user='root', \n",
    "    password='N@wid2003', \n",
    "    db='dma_iot_morefish_spark_farms_v3'\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Get the latest data from the database and store it in a pandas dataframe\n",
    "query = \"SELECT dvd_ph, dvd_temp, dvd_updated_at, dvd_do, dvd_dev_id FROM device_devicedata WHERE dvd_ph > 0 AND dvd_temp > 0 AND dvd_dev_id = 2 ORDER BY dvd_updated_at ASC\"\n",
    "df = pd.read_sql(query, connection)\n",
    "\n",
    "# Close the connection\n",
    "connection.close()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "excelData = pd.read_excel(\"../../../../../../Downloads/device_data_july13.xlsx\")\n",
    "excelData.to_csv(\"../datasets/Philly/device_data_july13.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'dvd_ph': 'ph', 'dvd_temp': 'temperature', 'dvd_updated_at': 'datetime', 'dvd_do': 'do_linreg'}, inplace=True)\n",
    "df['rounded_datetime'] = pd.to_datetime(df['datetime']).dt.round('H')\n",
    "# spark_weather = pd.read_csv(\"../../../../../../Downloads/sparkfarmsweather.csv\")\n",
    "spark_weather = pd.read_csv(\"../datasets/sparkfarmsweather.csv\")\n",
    "spark_weather['time'] = pd.to_datetime(spark_weather['time'])\n",
    "spark_weather.rename(columns={'time': 'datetime'}, inplace=True)\n",
    "# Merge the weather data with the main DataFrame based on 'rounded_datetime' column\n",
    "df = df.merge(spark_weather, left_on='rounded_datetime', right_on='datetime', how='left')\n",
    "df = df.drop(columns=['datetime_y'])\n",
    "df.rename(columns={'datetime_x': 'datetime', 'diffuse_radiation (W/m²)': 'light', 'relativehumidity_2m (%)': 'humid', 'temperature_2m (°C)': 'airtemp' }, inplace=True)\n",
    "# drop all NaN values inside diffuse radiation\n",
    "df = df.dropna(subset=['light'])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new features: depth, temperature^2, airtemp^2, temp*airtemp, depth*temp\n",
    "\n",
    "df['Depth'] = 1.2192\n",
    "df['temperature'] = pd.to_numeric(df['temperature'])\n",
    "df['temperature^2'] = df['temperature'] * df['temperature']\n",
    "df['airtemp^2'] = df['airtemp'] * df['airtemp']\n",
    "df['temp*airtemp'] = df['temperature'] * df['airtemp']\n",
    "df['depth*temp'] = df['Depth'] * df['temperature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_test = df[['temperature^2', 'airtemp^2','temp*airtemp', 'depth*temp', 'Depth', 'windspeed_10m (km/h)', 'light']]\n",
    "y_pred = poly_reg.predict(poly_features.transform(sf_test))\n",
    "df['do_poly'] = y_pred\n",
    "df[['do_linreg', 'do_poly']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "myDates = [datetime(2012,1,i+3) for i in range(10)]\n",
    "myValues = [5,6,4,3,7,8,1,2,5,4]\n",
    "fig, ax = plt.subplots()\n",
    "bet = df.loc[df['datetime'].between('2023-06-10', '2023-06-13')]\n",
    "ax.plot(bet['datetime'], bet['do_poly'].astype(float), 'bo')\n",
    "\n",
    "myFmt = DateFormatter(\"%m %d %H:%M\")\n",
    "ax.xaxis.set_major_formatter(myFmt)\n",
    "\n",
    "## Rotate date labels automatically\n",
    "fig.autofmt_xdate()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
